{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_rugpt3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fVBpXb9iHv44"
      },
      "outputs": [],
      "source": [
        "!rm -rf /usr/local/cuda\n",
        "!ln -s /usr/local/cuda-10.1 /usr/local/cuda"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export LD_LIBRARY_PATH=/usr/lib/"
      ],
      "metadata": {
        "id": "ksU2luuRIL6k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install clang-9 llvm-9 llvm-9-dev llvm-9-tools > /dev/null"
      ],
      "metadata": {
        "id": "2MHaecnNIN35"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\""
      ],
      "metadata": {
        "id": "pXkWs1NxIP_V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a075689-1420-4262-83e6-770a0151f60a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA version: 10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html  --quiet"
      ],
      "metadata": {
        "id": "GG1g0NrvIS6v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca695c59-63bb-4302-d9bb-940d0d467e6f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 735.4 MB 16 kB/s \n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 33.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 2.1 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.1+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==3.5.1 --quiet"
      ],
      "metadata": {
        "id": "21z6JH8ZIV4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69a50e5d-4dae-4506-f0af-bc3dc5968f59"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.3 MB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 42.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 30.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 43.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone  https://github.com/sberbank-ai/ru-gpts"
      ],
      "metadata": {
        "id": "0tLD3UbgIX2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1020aacd-e31a-414a-9ec3-8dc31e264205"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ru-gpts'...\n",
            "remote: Enumerating objects: 683, done.\u001b[K\n",
            "remote: Counting objects: 100% (178/178), done.\u001b[K\n",
            "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
            "remote: Total 683 (delta 110), reused 141 (delta 83), pack-reused 505\u001b[K\n",
            "Receiving objects: 100% (683/683), 413.81 KiB | 3.60 MiB/s, done.\n",
            "Resolving deltas: 100% (410/410), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models/"
      ],
      "metadata": {
        "id": "KKTrZlLYIaoi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# finetune"
      ],
      "metadata": {
        "id": "Fgzg4xE4Imrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export PYTHONPATH=${PYTHONPATH}:/ru-gpts/\n",
        "!CUDA_VISIBLE_DEVICES=0 python ru-gpts/pretrain_transformers.py \\\n",
        "    --output_dir=models/essays \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=sberbank-ai/rugpt3small_based_on_gpt2 \\\n",
        "    --do_train \\\n",
        "    --train_data_file=train.txt \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=valid.txt \\\n",
        "    --per_gpu_train_batch_size 1 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --num_train_epochs 4 \\\n",
        "    --block_size 1024 \\\n",
        "    --overwrite_output_dir"
      ],
      "metadata": {
        "id": "ISoRFQmMIdN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip weights.zip -r models"
      ],
      "metadata": {
        "id": "DWGt_yCtoVFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# model_name_or_path = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
        "model_name_or_path = 'models/essays'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).cuda()"
      ],
      "metadata": {
        "id": "lIacu37A9Cx0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "sent1 = \"<speaker1>Я пошел гулять.<speaker2>\"\n",
        "sent2 = \"<speaker1>Я чувствую обиду и злость за свой поступок.<speaker2>\"\n",
        "sent3 = \"<speaker1>Снег автомату рознь, а собака скользкая.<speaker2>\"\n",
        "sent4 = \"<speaker1>Скажи мне что-либо приятное.<speaker2>\"\n",
        "\n",
        "dataset = [sent1, sent2, sent3, sent4]\n",
        "\n",
        "for item in dataset:\n",
        "  text = item\n",
        "  time_start = time.time()\n",
        "  input_ids = tokenizer.encode(text, return_tensors=\"pt\").cuda()\n",
        "  out = model.generate(\n",
        "      input_ids.cuda(),\n",
        "      top_k=1,\n",
        "      top_p=0.95,\n",
        "      temperature=1.2,\n",
        "      num_return_sequences=1,\n",
        "      max_length=32,\n",
        "      no_repeat_ngram_size=3,\n",
        "      repetition_penalty=2.5)\n",
        "  generated_text = list(map(tokenizer.decode, out))[0]\n",
        "  print(\"calc time {}\".format(time.time() - time_start))\n",
        "  print(generated_text)\n",
        "  print(\"-----------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPJ9q443l8Yk",
        "outputId": "6b801d3b-cc89-41d4-cdf9-e94ad2ae9298"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calc time 0.5626204013824463\n",
            "<speaker1> Я пошел гулять. <speaker2> А я люблю готовить, особенно торты) <eos> <bos> \"Привет! Как дела? Чем занимаешься?)\"Добрый день!)\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calc time 0.4650156497955322\n",
            "<speaker1> Я чувствую обиду и злость за свой поступок. <speaker2> А я люблю животных, поэтому у меня есть собака) а ты? <eos> <bos> \"Привет! Как дела\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calc time 0.4066886901855469\n",
            "<speaker1> Снег автомату рознь, а собака скользкая. <speaker2> А я люблю гулять по парку и смотреть на звезды) <eos> <bos> \"Привет! Как дела\n",
            "-----------------------------------\n",
            "calc time 0.48810410499572754\n",
            "<speaker1> Скажи мне что-либо приятное. <speaker2> Я люблю петь, а ты? <eos> <bos> \"\n",
            "Привет! Как дела?) Чем занимаешься?) Расскажи\n",
            "-----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rl_y81Ufs1wA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}